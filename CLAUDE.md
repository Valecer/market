# Claude Agent Context

## Project Overview

**Name:** Marketbel - Unified Catalog System  
**Type:** Multi-service application for supplier price list management and product catalog

| Phase | Name | Status |
|-------|------|--------|
| 1 | Python Worker (Data Ingestion) | âœ… Complete |
| 2 | Bun API (REST API Layer) | âœ… Complete |
| 3 | React Frontend | âœ… Complete |
| 4 | Product Matching Pipeline | âœ… Complete |
| 5 | Frontend i18n | âœ… Complete |
| 6 | Admin Sync Scheduler | âœ… Complete |
| 7 | ML-Analyze Service (RAG Pipeline) | âœ… Complete |
| 8 | ML-Ingestion Integration (Courier Pattern) | âœ… Complete |
| 9 | Advanced Pricing & Categorization | ðŸš§ In Progress |

**Before implementation:** Use mcp context7 to collect up-to-date documentation.  
**For frontend:** Use i18n (add text to translation files in `public/locales/`).
**For frontend:** Use mcp 21st-dev/magic For design elements.

---

## Technology Stack

### Backend (Phases 1, 4, 6, 7)
- **Runtime:** Python 3.12+ (venv)
- **ORM:** SQLAlchemy 2.0+ AsyncIO
- **Queue:** arq (Redis-based)
- **Validation:** Pydantic 2.x
- **Matching:** RapidFuzz (Phase 4)

### ML-Analyze Service (Phase 7)
- **Framework:** FastAPI + uvicorn
- **RAG:** LangChain + LangChain-Ollama
- **LLM:** Ollama (nomic-embed-text for embeddings, llama3 for matching)
- **Vector DB:** pgvector (PostgreSQL extension)
- **File Parsing:** pymupdf4llm (PDF), openpyxl (Excel)
- **Database:** asyncpg (async PostgreSQL driver)

### API (Phase 2)
- **Runtime:** Bun
- **Framework:** ElysiaJS
- **ORM:** Drizzle ORM + node-postgres
- **Auth:** @elysiajs/jwt, bcrypt
- **Validation:** TypeBox

### Frontend (Phases 3, 5)
- **Framework:** React 18+ with TypeScript
- **Build:** Vite 5+
- **State:** TanStack Query v5
- **UI:** Radix UI Themes
- **Styling:** Tailwind CSS v4.1 (CSS-first, NO tailwind.config.js)
- **i18n:** react-i18next

### Infrastructure
- **Database:** PostgreSQL 16 (JSONB)
- **Cache/Queue:** Redis 7
- **Containers:** Docker Compose v2

---

## Project Structure

```
marketbel/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ python-ingestion/    # Phases 1, 4, 6, 8, 9 - Worker (Data Courier)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ db/models/   # SQLAlchemy ORM
â”‚   â”‚   â”‚   â”œâ”€â”€ parsers/     # Data source parsers
â”‚   â”‚   â”‚   â”œâ”€â”€ services/    # Matching, ML client (Phase 8)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ml_client.py      # HTTP client for ml-analyze
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ job_state.py      # Redis job state management
â”‚   â”‚   â”‚   â””â”€â”€ tasks/       # arq tasks
â”‚   â”‚   â”‚       â”œâ”€â”€ download_tasks.py # File download + ML trigger (Phase 8)
â”‚   â”‚   â”‚       â”œâ”€â”€ ml_polling_tasks.py # ML job status polling (Phase 8)
â”‚   â”‚   â”‚       â”œâ”€â”€ cleanup_tasks.py  # Shared file cleanup (Phase 8)
â”‚   â”‚   â”‚       â””â”€â”€ retry_tasks.py    # Failed job retry (Phase 8)
â”‚   â”‚   â””â”€â”€ migrations/      # Alembic
â”‚   â”œâ”€â”€ ml-analyze/          # Phase 7 - AI Service (Intelligence)
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ api/         # FastAPI endpoints
â”‚   â”‚       â”œâ”€â”€ db/          # Repositories, models
â”‚   â”‚       â”œâ”€â”€ ingest/      # File parsers (PDF, Excel)
â”‚   â”‚       â”œâ”€â”€ rag/         # VectorService, MergerAgent
â”‚   â”‚       â””â”€â”€ services/    # Business logic orchestration
â”‚   â”œâ”€â”€ bun-api/             # Phase 2 - API
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ controllers/ # auth/, catalog/, admin/
â”‚   â”‚       â”œâ”€â”€ services/    # Business logic, job.service.ts (Phase 8)
â”‚   â”‚       â””â”€â”€ db/          # Drizzle schemas
â”‚   â””â”€â”€ frontend/            # Phases 3, 5, 8, 9 - UI
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ components/  # catalog/, admin/, cart/, shared/
â”‚           â”‚   â””â”€â”€ admin/
â”‚           â”‚       â”œâ”€â”€ JobPhaseIndicator.tsx  # Phase 8
â”‚           â”‚       â””â”€â”€ SupplierStatusTable.tsx # Phase 8 enhancements
â”‚           â”œâ”€â”€ pages/       # Route components
â”‚           â”œâ”€â”€ hooks/       # Custom hooks
â”‚           â”‚   â””â”€â”€ useRetryJob.ts  # Phase 8
â”‚           â””â”€â”€ lib/         # API client, utils
â”œâ”€â”€ uploads/                 # Shared Docker volume for file handoff
â”œâ”€â”€ specs/                   # Feature specifications
â”‚   â”œâ”€â”€ 001-data-ingestion-infra/
â”‚   â”œâ”€â”€ 002-api-layer/
â”‚   â”œâ”€â”€ 003-frontend-app/
â”‚   â”œâ”€â”€ 004-product-matching-pipeline/
â”‚   â”œâ”€â”€ 005-frontend-i18n/
â”‚   â”œâ”€â”€ 006-admin-sync-scheduler/
â”‚   â”œâ”€â”€ 007-ml-analyze/
â”‚   â”œâ”€â”€ 008-ml-ingestion-integration/
â”‚   â””â”€â”€ 009-advanced-pricing-categories/
â””â”€â”€ docker-compose.yml
```

---

## Code Style & Patterns

### Python (Worker)
- Async/await for all I/O
- Type hints required
- Pydantic models for validation
- SQLAlchemy 2.0 style (no legacy Query API)
- Use `patch.object()` for mocking (never direct assignment)

### TypeScript (Bun API)
- Strict mode, no `any`
- Controllers â†’ Services â†’ Repositories (SOLID)
- TypeBox schemas for validation
- Feature-based structure

### React (Frontend)
- Strict mode, no `any`
- Components â†’ Hooks â†’ API Client
- TanStack Query for server state
- Tailwind v4.1: `@import "tailwindcss"` + `@theme` blocks
- All UI text via i18n: `t('namespace.key')`

---

## Quick Commands

```bash
# Docker
docker-compose up -d
docker-compose logs -f worker|bun-api|frontend|ml-analyze

# Python Worker
cd services/python-ingestion
source venv/bin/activate
pytest tests/ -v --cov=src
alembic upgrade head

# ML-Analyze Service
cd services/ml-analyze
source venv/bin/activate
uvicorn src.api.main:app --reload --port 8001
pytest tests/ -v --cov=src

# Bun API
cd services/bun-api
bun install && bun --watch src/index.ts
bun test

# Frontend
cd services/frontend
bun install && bun run dev
bun run type-check
```

---

## Phase 6: Admin Sync Scheduler

### Purpose
Centralized admin panel for monitoring and controlling supplier data ingestion with automated scheduled synchronization from Master Google Sheet.

### Key Features
- **Master Sheet Parser:** Reads supplier configurations from central Google Sheet
- **Supplier Sync:** Create/update suppliers in DB based on Master Sheet
- **Cascading Parse:** Auto-enqueue parsing for all active suppliers after sync
- **Scheduled Sync:** Default 8h interval via `SYNC_INTERVAL_HOURS`
- **Admin UI:** Status dashboard, manual sync trigger, live log stream

### Key Files (Phase 6)
- `services/python-ingestion/src/parsers/master_sheet_parser.py`
- `services/python-ingestion/src/tasks/sync_tasks.py`
- `services/bun-api/src/controllers/admin/sync.controller.ts`
- `services/frontend/src/pages/admin/IngestionControlPage.tsx`

### Spec Reference
- `/specs/006-admin-sync-scheduler/spec.md`

---

## Phase 7: ML-Analyze Service

### Purpose
AI-powered service for parsing complex unstructured supplier data (PDFs with tables, Excel with merged cells) using RAG (Retrieval-Augmented Generation) pipeline with local LLM for intelligent product matching.

### Key Features
- **Complex File Parsing:** Handles PDF tables and Excel merged cells using pymupdf4llm + openpyxl
- **Vector Embeddings:** Generates 768-dim semantic embeddings with nomic-embed-text (Ollama)
- **Semantic Search:** pgvector with IVFFLAT indexing for cosine similarity search
- **LLM Matching:** llama3 (Ollama) for reasoning-based product matching with confidence scores
- **Confidence Thresholds:** >90% auto-match, 70-90% review queue, <70% reject

### API Endpoints
- `POST /analyze/file` - Trigger file analysis (returns job_id)
- `GET /analyze/status/:job_id` - Poll job progress and results
- `POST /analyze/merge` - Trigger batch product matching
- `GET /health` - Service health check (DB, Ollama, Redis)

### Key Files (Phase 7)
- `services/ml-analyze/src/api/main.py` - FastAPI application
- `services/ml-analyze/src/ingest/excel_strategy.py` - Excel parser
- `services/ml-analyze/src/ingest/pdf_strategy.py` - PDF parser
- `services/ml-analyze/src/rag/vector_service.py` - Embedding + vector search
- `services/ml-analyze/src/rag/merger_agent.py` - LLM-based matching

### Spec Reference
- `/specs/007-ml-analyze/spec.md`

---

## Phase 8: ML-Ingestion Integration (Courier Pattern)

### Purpose
Refactored ingestion pipeline where `python-ingestion` acts as a **data courier** (fetching/downloading files) and delegates all parsing intelligence to the `ml-analyze` service for superior data extraction quality.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Admin UI (React)                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Upload File  â”‚  â”‚ JobPhaseIndicator â”‚  â”‚ Retry Button (failed jobs)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚ Poll status                â”‚
          â–¼                   â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Bun API (ElysiaJS)                                 â”‚
â”‚  POST /admin/suppliers    GET /admin/sync/status    POST /admin/jobs/:id/retry â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ Enqueue task                                  â”‚ Enqueue retry
          â–¼                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              Redis Queue                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ job:{job_id}   â”‚  â”‚ download tasks   â”‚  â”‚ retry_job_task             â”‚   â”‚
â”‚  â”‚ (phase, progress)â”‚ â”‚ poll_ml_status   â”‚  â”‚ cleanup_shared_files       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Python Worker (Data Courier)                            â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ download_and_trigger_ml task                                          â”‚   â”‚
â”‚  â”‚   1. Download file (Google Sheets â†’ XLSX, CSV, Excel)                 â”‚   â”‚
â”‚  â”‚   2. Save to /shared/uploads/{supplier_id}_{timestamp}_{filename}     â”‚   â”‚
â”‚  â”‚   3. Write metadata sidecar (.meta.json)                              â”‚   â”‚
â”‚  â”‚   4. HTTP POST â†’ ml-analyze /analyze/file                             â”‚   â”‚
â”‚  â”‚   5. Update Redis job state: phase=downloading â†’ analyzing            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ poll_ml_status   â”‚  â”‚ cleanup_files    â”‚  â”‚ retry_job_task         â”‚     â”‚
â”‚  â”‚ (every 10s)      â”‚  â”‚ (every 6h, 24h   â”‚  â”‚ (max 3 retries)        â”‚     â”‚
â”‚  â”‚                  â”‚  â”‚  TTL)            â”‚  â”‚                        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                 â”‚                 â”‚
                   â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shared Volume       â”‚  â”‚  HTTP REST       â”‚  â”‚  PostgreSQL              â”‚
â”‚  /shared/uploads     â”‚  â”‚  (Internal       â”‚  â”‚  (Results)               â”‚
â”‚  (Docker volume)     â”‚  â”‚   Docker net)    â”‚  â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                       â”‚                          â–²
           â”‚ Read file             â”‚ POST /analyze/file       â”‚ Save items
           â–¼                       â–¼                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ML-Analyze Service (Intelligence)                       â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ /analyze/file endpoint                                              â”‚     â”‚
â”‚  â”‚   1. Read file from shared volume                                   â”‚     â”‚
â”‚  â”‚   2. Parse (PDF via pymupdf4llm, Excel via openpyxl)                â”‚     â”‚
â”‚  â”‚   3. Generate embeddings (nomic-embed-text via Ollama)              â”‚     â”‚
â”‚  â”‚   4. Match products (llama3 via Ollama, pgvector similarity)        â”‚     â”‚
â”‚  â”‚   5. Save to supplier_items table                                   â”‚     â”‚
â”‚  â”‚   6. Return job status with progress                                â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
[Admin Upload] â†’ [Bun API] â†’ [Redis: job created] â†’ [Worker picks up]
                                                           â”‚
                                                           â–¼
[Google Sheets API] â† authenticate â† [Worker: download_task]
         â”‚                                    â”‚
         â”‚ export XLSX                        â”‚ save file
         â–¼                                    â–¼
[/shared/uploads/file.xlsx]           [Redis: phase=downloading]
         â”‚                                    â”‚
         â”‚                                    â”‚ trigger ML
         â–¼                                    â–¼
[ml-analyze reads file] â† HTTP POST â† [Worker: ml_client]
         â”‚                                    â”‚
         â”‚ parse + match                      â”‚ poll status
         â–¼                                    â–¼
[PostgreSQL: supplier_items] â†’ [Redis: phase=complete] â†’ [Frontend polls]
```

### Key Features
- **Courier Pattern:** `python-ingestion` downloads files only; no parsing logic
- **Shared Volume:** Zero-copy file handoff via Docker volume (`/shared/uploads`)
- **Multi-Phase Status:** Downloading â†’ Analyzing â†’ Matching â†’ Complete/Failed
- **ML Toggle:** Per-supplier `use_ml_processing` flag (default: true)
- **Retry Logic:** Failed jobs can be retried via Admin UI (max 3 attempts)
- **File Cleanup:** Automatic 24-hour TTL cleanup cron task

### API Endpoints (Phase 8)
- `POST /admin/jobs/:id/retry` - Retry a failed job (admin only)
- `GET /admin/sync/status` - Extended with `jobs` array and `current_phase`

### Environment Variables
| Variable | Default | Description |
|----------|---------|-------------|
| `ML_ANALYZE_URL` | `http://ml-analyze:8001` | ML service URL |
| `USE_ML_PROCESSING` | `true` | Global toggle for ML pipeline |
| `ML_POLL_INTERVAL_SECONDS` | `5` | Status polling interval |
| `MAX_FILE_SIZE_MB` | `50` | Maximum upload file size |
| `FILE_CLEANUP_TTL_HOURS` | `24` | File retention before cleanup |

### Key Files (Phase 8)
- `services/python-ingestion/src/services/ml_client.py` - HTTP client for ml-analyze
- `services/python-ingestion/src/services/job_state.py` - Redis job state helpers
- `services/python-ingestion/src/tasks/download_tasks.py` - Download + ML trigger
- `services/python-ingestion/src/tasks/ml_polling_tasks.py` - ML status polling
- `services/python-ingestion/src/tasks/cleanup_tasks.py` - File cleanup cron
- `services/python-ingestion/src/tasks/retry_tasks.py` - Job retry logic
- `services/bun-api/src/services/job.service.ts` - Job retry service
- `services/frontend/src/components/admin/JobPhaseIndicator.tsx` - Phase UI
- `services/frontend/src/hooks/useRetryJob.ts` - Retry mutation hook

### Spec Reference
- `/specs/008-ml-ingestion-integration/spec.md`
- `/specs/008-ml-ingestion-integration/plan.md`
- `/docs/adr/008-courier-pattern.md` - Architecture Decision Record

---

## Phase 9: Advanced Pricing & Categorization

### Purpose
Enable advanced pricing models by supporting dual pricing (retail and wholesale) with currency tracking on products. Leverages existing category hierarchy for product organization.

### Key Features
- **Dual Pricing:** Products store both `retail_price` (end-customer) and `wholesale_price` (bulk/dealer)
- **Currency Tracking:** ISO 4217 currency code per product (e.g., USD, EUR, RUB)
- **Exact Decimals:** All monetary fields use `DECIMAL(10,2)` to avoid floating-point errors
- **Category Hierarchy:** Existing adjacency list pattern (`parent_id`) supports infinite nesting
- **Non-Negative Constraints:** Database CHECK constraints prevent negative prices

### New Product Fields

| Field | Type | Nullable | Description |
|-------|------|----------|-------------|
| `retail_price` | `DECIMAL(10,2)` | Yes | End-customer price |
| `wholesale_price` | `DECIMAL(10,2)` | Yes | Bulk/dealer price |
| `currency_code` | `VARCHAR(3)` | Yes | ISO 4217 currency code |

**Note:** These are **canonical product-level prices**, distinct from `min_price` which is an aggregate of supplier prices.

### API Endpoints (Phase 9)
- `GET /api/products` - Returns pricing fields in response
- `GET /api/products/{id}` - Returns pricing fields in detail
- `PATCH /api/admin/products/{id}/pricing` - Update pricing (admin only)

### Key Files (Phase 9)
- `services/python-ingestion/migrations/versions/009_add_pricing_fields.py` - Migration
- `services/python-ingestion/src/db/models/product.py` - SQLAlchemy model (updated)
- `services/python-ingestion/src/models/product_pricing.py` - Pydantic validation
- `services/bun-api/src/db/schema/schema.ts` - Drizzle schema (updated)
- `services/bun-api/src/controllers/admin/products.controller.ts` - Pricing endpoint
- `services/frontend/src/components/shared/PriceDisplay.tsx` - Price formatting
- `services/frontend/src/components/catalog/ProductCard.tsx` - List display
- `services/frontend/src/components/catalog/ProductDetail.tsx` - Detail display

### Spec Reference
- `/specs/009-advanced-pricing-categories/spec.md`
- `/specs/009-advanced-pricing-categories/plan.md`
- `/specs/009-advanced-pricing-categories/tasks.md`

---

## Database Schema (Key Tables)

| Table | Description |
|-------|-------------|
| `suppliers` | External data sources (google_sheets, csv, excel) |
| `products` | Internal catalog with dual pricing (retail/wholesale + currency) |
| `supplier_items` | Raw supplier data with JSONB characteristics |
| `product_embeddings` | Vector embeddings (768-dim) for semantic search (Phase 7) |
| `price_history` | Time-series price tracking |
| `parsing_logs` | Structured error logging |
| `users` | Authentication (roles: sales, procurement, admin) |
| `match_review_queue` | Pending matches for human review (Phase 4) |
| `categories` | Hierarchical product categories (adjacency list via `parent_id`) |

---

## Key Architectural Decisions

1. **SOLID everywhere:** Controllers handle HTTP only, Services handle logic, Repositories handle data
2. **Error isolation:** Per-row logging to `parsing_logs` - worker never crashes on bad data
3. **Type safety:** End-to-end types from DB â†’ API â†’ Frontend
4. **KISS:** No WebSockets (polling), no Redux, no complex abstractions
5. **Queue-based:** Python worker consumes tasks, API publishes
6. **Courier Pattern (Phase 8):** `python-ingestion` acts as data courier; `ml-analyze` handles intelligence (see [ADR-008](/docs/adr/008-courier-pattern.md))

---

## Spec References

Each phase has detailed documentation in `/specs/00X-name/`:
- `spec.md` - Feature specification
- `plan/research.md` - Technology decisions
- `plan/data-model.md` - Schema definitions
- `plan/quickstart.md` - 15-minute setup guide
- `plan/contracts/` - API contracts (JSON schemas)
